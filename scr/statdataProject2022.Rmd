---
title: "R Notebook"
output: html_notebook
---

```{r}
library(ggplot2)
library(robustbase)
library(rrcov)
library(reshape2)
library(cluster)
library(stats)
library(dbscan)
```

# EDA 
## loading data set
```{r}
cars_data<-read.csv("~/GitHub/Statistical-Data-Science-Project-Academic-year-2022-2023/cars_data")
```

## generate subset
```{r}
set.seed(180901)
cars_data$euro_standard <- as.factor(cars_data$euro_standard)
cars_data$transmission_type <- as.factor(cars_data$transmission_type)
cars_data$fuel_type <- as.factor(cars_data$fuel_type)
subset_cars = cars_data[sample(nrow(cars_data),3000,replace=FALSE),]
```
I reviewed the data set in a csv viewer to get a feel for it. Noise level has weird rounding thing going on.

```{r}
# what I expect:
ggplot(subset_cars,aes(factor((urban_metric*10)%%10)))+ geom_bar()
ggplot(subset_cars,aes(factor((combined_metric*10)%%10)))+ geom_bar()
# weird rounding:
ggplot(subset_cars,aes(factor((noise_level*10)%%10)))+ geom_bar()
ggplot(subset_cars,aes(factor((co_emissions*10)%%10)))+ geom_bar()
ggplot(subset_cars,aes(factor((nox_emissions*10)%%10)))+ geom_bar()
# normal rounding:
ggplot(subset_cars,aes(factor((co2*10)%%10)))+ geom_bar()


```
It would be interesting to see if rounding can be predicted or is useful.
## checking for duplicates

```{r}
#View(subset_cars[duplicated(subset_cars),])
sum(duplicated(subset_cars))
sum(duplicated(subset_cars["model"]))
sum(duplicated(subset_cars[c("urban_metric", "noise_level", "co2")]))
```
OK some duplicates. In theory it shouldn't be impossible ... but IDK if this was intended when gathering the data ... A more in depth study of duplicates could be done using clustering techniques.


## Are there missing values?

```{r}
sum(is.na(subset_cars))
```

## what can I plot? 

```{r}
names(subset_cars)

```

```{r}
str(subset_cars)
```

## univariate plots for factor types
```{r}
# (for future me) for loop doesn't work and dont plot model
ggplot(subset_cars ,aes(manufacturer)) + geom_bar(stat ="count")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) 

ggplot(subset_cars ,aes(euro_standard)) + geom_bar(stat ="count")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) 

ggplot(subset_cars ,aes(transmission_type)) + geom_bar(stat ="count")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) 

ggplot(subset_cars ,aes(fuel_type)) + geom_bar(stat ="count")+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) 


```
Everything looks balanced minus the manufacturer. From our humble car knowledge we think that there are major differences between cars from different manufacturers (you shouldn't compare a Lamborghini with a mini it just has other stats). This probably gives a problem with balancing (whatever we mean). We are not sure that the counts represents the amount of cars in Europe? Later we'll look into it.

## plots for numerical types

```{r}
ggplot(subset_cars,aes(engine_capacity))+ geom_histogram()
ggplot(subset_cars,aes(urban_metric))+ geom_histogram()
ggplot(subset_cars,aes(extra_urban_metric))+ geom_histogram()
ggplot(subset_cars,aes(combined_metric))+ geom_histogram()
selectNormalNoise = (60<subset_cars$noise_level) & (subset_cars$noise_level < 80)
ggplot(subset_cars[selectNormalNoise,],aes(noise_level))+ geom_histogram(bins = 15)
ggplot(subset_cars,aes(co2))+ geom_histogram()
ggplot(subset_cars,aes(co_emissions))+ geom_histogram()
selectNormalNox = (subset_cars$nox_emissions < 1000)
ggplot(subset_cars[selectNormalNox, ],aes(nox_emissions))+ geom_histogram()
```
Everything looks skewed to the right minus the noise emissions (maybe this is because it is on a log scale?). I'll try transforming the noise. Then some outliers (points that ruined my plots so we filterd them out).
```{r}
selectBadRows <- !selectNormalNoise | !selectNormalNox
selectNormalRows <- !selectBadRows
#why doesn't this work? OK used cars_data instead of subset ...
subset.bad <- subset_cars[selectBadRows, ]
subset.trans <- subset_cars[selectNormalRows, ]
subset.trans$noise_level <- 1.26^(subset.trans$noise_level-70)
```

```{r}
ggplot(subset.trans,aes(engine_capacity))+ geom_histogram()
ggplot(subset.trans,aes(urban_metric))+ geom_histogram()
ggplot(subset.trans,aes(extra_urban_metric))+ geom_histogram()
ggplot(subset.trans,aes(combined_metric))+ geom_histogram()
ggplot(subset.trans,aes(noise_level))+ geom_histogram(bins = 17)
ggplot(subset.trans,aes(co2))+ geom_histogram()
ggplot(subset.trans,aes(co_emissions))+ geom_histogram()
ggplot(subset.trans,aes(nox_emissions))+ geom_histogram()
```

```{r}
subset.trans[order(subset.trans$noise_level,decreasing = TRUE), c("manufacturer","noise_level" )][1:20, ]
```
## plots for comparing variables
```{r}
ggplot(subset.trans, aes(x = engine_capacity, y = co2, color = transmission_type)) + 
  geom_jitter(alpha = 0.3, width = 100) +
  facet_grid(~fuel_type)

ggplot(subset.trans, aes(x = engine_capacity, y = nox_emissions, color = transmission_type)) + 
  geom_jitter(alpha = 0.3, width = 100) +
  facet_grid(~fuel_type)

ggplot(subset.trans, aes(x = engine_capacity, y = urban_metric, color = fuel_type)) + 
  geom_jitter(alpha = 0.3, width = 100)+
  scale_color_manual(values=c("purple", "green"))

ggplot(subset.trans, aes(x = urban_metric, y = co_emissions, color = euro_standard)) + 
  geom_jitter(alpha = 0.3 ) 

ggplot(subset.trans, aes(x = co2, y = urban_metric, color = combined_metric, alpha = extra_urban_metric)) + 
  geom_jitter( width=30)+
  scale_colour_gradientn(colours=rainbow(4))

# a non linear jitter would be more appropriate ...
ggplot(subset.trans, aes(x = engine_capacity, y = noise_level, color = transmission_type)) +
  geom_jitter(alpha = 0.3, width=50, height = 0.3 ) +
  facet_grid(~fuel_type)
```
These plot show that there are a lot of (near) linear relationships between variables and that the discrete variables are important. It makes it easy to find interesting rows.
## intersting rows
```{r}
subset.trans[subset.trans$engine_capacity> 8000,]
subset.trans[subset.trans$nox_emissions> 200 & subset.trans$fuel_type == "Petrol",]
subset.trans[subset.trans$co2> 440,]
```

# PCA
## splitting into training and validation
```{r}
subset.transNoText <- subset.trans[ , -which(names(subset.trans) %in% c("manufacturer"
,"model","description" ))]
subset.transNoText <- na.omit(subset.transNoText)
```

```{r}
# it looks complicated because I had a bug and now I dont want to touch it ...
set.seed(180901)
train_ind <- sample.int(n=nrow(subset.trans),size=1500,replace=F)
subset.transNoText$inTrain = FALSE
subset.transNoText[train_ind,]$inTrain = TRUE
nrow(na.omit(subset.transNoText))
Xtrain = subset.transNoText[subset.transNoText$inTrain,]
Xtrain <- na.omit(Xtrain)
Xval = subset.transNoText[!(subset.transNoText$inTrain),]
Xval <- na.omit(Xval[!is.na(Xval),])
nrow(Xtrain)
nrow(Xval)
```
## PCA on training


```{r}
continousCols <- c("engine_capacity", "urban_metric", "extra_urban_metric", "combined_metric","co2", "noise_level", "co_emissions", "nox_emissions")

ggplot(melt(round(cov(Xtrain[,continousCols]),1)), 
       aes(x = Var1, y= Var2, fill= value))+
  geom_tile()+
  geom_text(aes(Var2, Var1, label = value),color = "white", size = 4)+
  theme(axis.text.x=element_text(angle=45, hjust=1,vjust=1)) +
  scale_fill_gradient(low = "red", high = "green")+
  ggtitle("covariance heatmap of continous var Xtrain")

ggplot(melt(round(covMcd(Xtrain[,continousCols], cor = TRUE)$cov,1)), 
       aes(x = Var1, y= Var2, fill= value))+
  geom_tile()+
  geom_text(aes(Var2, Var1, label = value),color = "white", size = 4)+
  theme(axis.text.x=element_text(angle=45, hjust=1,vjust=1)) +
  scale_fill_gradient(low = "red", high = "green")+
  ggtitle("robust cov heatmap of continous var Xtrain")

ggplot(melt(round(cor(Xtrain[,continousCols]),1)), 
       aes(x = Var1, y= Var2, fill= value))+
  geom_tile()+
  geom_text(aes(Var2, Var1, label = value),color = "white", size = 4)+
  theme(axis.text.x=element_text(angle=45, hjust=1,vjust=1)) +
  scale_fill_gradient(low = "red", high = "green")+
  ggtitle("cor heatmap of continous var Xtrain")

ggplot(melt(round(covMcd(Xtrain[,continousCols], cor = TRUE)$cor,1)), 
       aes(x = Var1, y= Var2, fill= value))+
  geom_tile()+
  geom_text(aes(Var2, Var1, label = value),color = "white", size = 4)+
  theme(axis.text.x=element_text(angle=45, hjust=1,vjust=1)) +
  scale_fill_gradient(low = "red", high = "green")+
  ggtitle("robust cor heatmap of continous var Xtrain")


```

From the variance heatmap it is clearly seen that pca on the covariance matrix
would be dominated by engine capacity therefore we have to scale the data (or use
the correlation) before doing PCA.


```{r}
{
Xtrain.pca.unscaled = PcaClassic(Xtrain[,continousCols])
print(summary(Xtrain.pca.unscaled))
Xtrain.pca.unscaled$loadings
}
```
Most of the variance gets captured by the first PC and the loadings aren't interesting because
of the poor scaling.

```{r}
Xtrain.pca.scaled = PcaClassic(Xtrain[,continousCols],scale =TRUE,crit.pca.distances=0.99)
summary(Xtrain.pca.scaled)
Xtrain.pca.scaled$loadings
```
In would save 3 PC, the first 2 for sure because they capture a lot variances and
the third because it is basically the noise level (look loadings) which is a variable
that sticks out.

## biplots PCA
```{r}
biplot(Xtrain.pca.scaled,c(1,2))
biplot(Xtrain.pca.scaled,c(1,3))
biplot(Xtrain.pca.scaled,c(2,3))
```
The biplots aren't very clear and we can't recognize any categorical variables in
them. Let's plot them again.

```{r}
# plotting scores
score_plot <- data.frame(Xtrain.pca.scaled$scores)
score_plot$fuel <-Xtrain[,"fuel_type"]
score_plot$type <-Xtrain[,"transmission_type"]
score_plot$euro <-Xtrain[,"euro_standard"]
score_plot$combined <- interaction(score_plot$fuel,score_plot$type)
ggplot(score_plot ,aes(PC1,PC2, color = fuel)) + geom_point(alpha = 0.5)
ggplot(score_plot ,aes(PC1,PC2, color = type)) + geom_point(alpha = 0.5)
ggplot(score_plot ,aes(PC1,PC2, color = euro)) + geom_point(alpha = 0.5)
ggplot(score_plot ,aes(PC1,PC2, color = combined)) + geom_point(alpha = 0.5)
ggplot(score_plot ,aes(PC2,PC3)) + geom_point(alpha = 0.5)
ggplot(score_plot ,aes(PC1,PC3)) + geom_point(alpha = 0.5)
```
We could only recognieze the fuel type out of the principal components.

## Compare to robust PCA


So we do robust PCA with PcaHubert 
```{r}
Xtrain.robpca <-PcaHubert(Xtrain[,continousCols], k=3,scale= mad,crit.pca.distances=0.99)
summary(Xtrain.robpca)
Xtrain.robpca$loadings
```
Looks very similar to not robust PCA maybe the loading of nox emissions
is a little bit of.

Now we're going to use robust PCA to check for PCA outliers:
```{r}
plot(Xtrain.pca.scaled,pch=19)
plot(Xtrain.robpca,pch=19)
```

```{r}
bad_leverage_points <- c("7036","2332","8906","36079")
good_leverage_points <- c("12481","36784")
print(subset.trans[bad_leverage_points,])
print(subset.trans[good_leverage_points,])
```

There are a lot of bad leverage points so we choose to continue with the robust PCA.

## scores of validation set

```{r}
Xval.robpca.scaled <- scale(Xval[,continousCols],center = Xtrain.robpca$center, scale= Xtrain.robpca$scale)
Xval.robpca.scores <- Xval.robpca.scaled %*% Xtrain.robpca$loadings
Xval.robpca.pred <- t(t(Xval.robpca.scores%*%t(Xtrain.robpca$loadings)) + Xtrain.robpca$center/Xtrain.robpca$scale)

Xtrain.robpca.scaled <- scale(Xtrain[,continousCols],center = Xtrain.robpca$center, scale= Xtrain.robpca$scale)
Xtrain.robpca.scores <- Xtrain.robpca.scaled %*% Xtrain.robpca$loadings
Xtrain.robpca.pred <- t(t(Xtrain.robpca.scores%*%t(Xtrain.robpca$loadings)) + Xtrain.robpca$center/Xtrain.robpca$scale)
```

## outlier map validation set
To make an outlier map we need the orthogonal and score distances.
```{r}
euclnorm <- function(y) sqrt(sum(y^2))
Xval.robpca.od <- apply(Xval.robpca.scaled - Xval.robpca.pred, 1, euclnorm)
Xval.robpca.sd <- sqrt(mahalanobis(Xval.robpca.scores, center = 0, diag(Xtrain.robpca$eigenvalues)))

Xtrain.robpca.od <- apply(Xtrain.robpca.scaled - Xtrain.robpca.pred, 1, euclnorm)
Xtrain.robpca.sd <- sqrt(mahalanobis(Xtrain.robpca.scores, center = 0, diag(Xtrain.robpca$eigenvalues)))

outlier_map <- data.frame(
  c(Xval.robpca.od,Xtrain.robpca.od),
  c(Xval.robpca.sd,Xtrain.robpca.sd),
  c(rep("Xval", 1500), rep("Xtrain",1500)))
names(outlier_map) <- c("od","sd","set")
ggplot(outlier_map,aes(sd,od, color = set)) + 
  geom_point(alpha = 0.3, size = 1)+
  ggtitle("outlier map Xval vs Xtrain")

```
This outlier map isn't the same as from the robust PCA. As you can see 
the distribution of the Xval is barely different from the Xtrain on the outlier
map this validates the robust PCA.

# Clustering
## partitioning cluster analysis
###kmeans
```{r}
for (amount_centers in 1:6) {
  Xtrain.kmeans <-kmeans(Xtrain.robpca.scaled, amount_centers)
  pairs(Xtrain.robpca.scores, col=Xtrain.kmeans$cluster)
}
```

### pam
```{r}
for (amount_centers in 1:6) {
  Xtrain.pam <-pam(Xtrain.robpca.scaled, k = amount_centers, metric="manhattan")
  pairs(Xtrain.robpca.scores, col=Xtrain.pam$clustering)
}
```
### conclusion
The only categorical variable you can recognize on the score plot was fuel
the other if you would plot them are overlapping ruling them out. Meaning
neither pam or kmeans recogized any categorical variables. 

We would go with 1 cluster here. 

## hierarchical clustering observations
TODO : do all types of clustering
TODO : hdbscan
### hdbscan
```{r}
for (pts in c(5,10,15,20,25)) {
  Xtrain.hdbscan <- hdbscan(Xtrain.robpca.scaled,minPts = pts)
  pairs(Xtrain.robpca.scores, col=Xtrain.hdbscan$cluster, main =pts)
  plot(Xtrain.hdbscan)
}
```
```{r}
```

### hclust
```{r}
for (mt in c("average","complete","ward.D")) {
  Xtrain.hclust <- hclust(dist(Xtrain.robpca.scaled),method =mt)
  plot(Xtrain.hclust, main = mt)
  rect.hclust(Xtrain.hclust, k = 7)
  for (amount_cluster in c(2,3,4)){
  pairs(Xtrain.robpca.scores, col=cutree(Xtrain.hclust,k=amount_cluster), main = mt)
  }
}
```


## hierarchical clustering variables

```{r}
for (mt in c("average","complete","ward")) {
  Xtrain.var.agnes <- agnes(cor(Xtrain[,continousCols]), method = mt)
  plot(Xtrain.var.agnes, main = mt)
  rect.hclust(Xtrain.var.agnes, k = 3)
}
```

## clustering that corresponds to fuel
Hclust with ward found clustering that correspond with fuel type.
```{r}
#Xtrain.hclust.ward.dendo <-as.dendrogram(hclust(dist(Xtrain.robpca.scaled), method = "ward.D"))
#Xtrain.var.agnes.ward.dendo <- as.dendrogram(agnes(cor(Xtrain[,continousCols]), method = "ward"))

Xtrain.hclust.ward <-cutree(hclust(dist(Xtrain.robpca.scaled), method = "ward.D"), k = 2)
Xtrain.var.agnes.ward <- cutree(agnes(cor(Xtrain[,continousCols]), method = "ward"),k=3)

Xtrain.sort_cluster <- Xtrain[,continousCols][order(Xtrain.hclust.ward),order(Xtrain.var.agnes.ward)]
Xtrain.sort_cluster.scaled <- Xtrain.robpca.scaled[order(Xtrain.hclust.ward),order(Xtrain.var.agnes.ward)]
#Xtrain.sort_cluster.melt <- melt(Xtrain.sort_cluster)
#Xtrain.sort_cluster.melt$idu <- as.numeric(row.names(Xtrain.sort_cluster.melt))
#ggplot(Xtrain.sort_cluster.melt,aes(variable,y =idu, fill = value))+geom_tile()

heatmap(data.matrix(Xtrain.sort_cluster),Rowv = NA,Colv = NA )
heatmap(data.matrix(Xtrain.sort_cluster.scaled),Rowv = NA,Colv = NA)

```
On the heatmap you can clearly see the conditional distribution on fuel and they are similar for similar variables.

```{r}
Xtrain.hdbscan.15 <-hdbscan(Xtrain.robpca.scaled,minPts = 15)
Xtrain.sort_cluster.scaled.hdbscan <- Xtrain.robpca.scaled[order(Xtrain.hdbscan.15$cluster),order(Xtrain.var.agnes.ward)]
heatmap(data.matrix(Xtrain.sort_cluster.scaled.hdbscan),Rowv = NA,Colv = NA)
```


# Linear regression

## linear model
TODO : lasso of lar
(response is co2)


## assumptions linear model

## ANOVA

## metrics

## confidence interval

# Classification 

## LDA and QDA

## knn

## preffered model















